{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0_LvL0ahEcS",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read data, load packages and utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y23RUjjjhEcS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip -q install mead-baseline gensim\n",
    "!pip -q install git+https://github.com/MeMartijn/updated-sklearn-crfsuite.git#egg=sklearn_crfsuite\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from baseline.utils import to_chunks\n",
    "from sklearn_crfsuite.metrics import flat_classification_report, flat_f1_score\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow.keras.optimizers\n",
    "from sklearn.model_selection import train_test_split \n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KNHHH_cwcTga",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def read_file(f):\n",
    "    data = open(f,'r').readlines()[1:]\n",
    "    row_id = [i.split('\\t')[0].strip() for i in data]\n",
    "    data = [i.split('\\t')[1].strip().split(' ') for i in data]\n",
    "    return row_id,data\n",
    "\n",
    "# Util function\n",
    "def read_conll_file(_file):\n",
    "    all_sentences = []\n",
    "    all_sentence_ids = [] \n",
    "    all_labels = []\n",
    "    sentence = []\n",
    "    labels = []\n",
    "    for line in tqdm(open(_file), desc=f\"reading {_file}\"):\n",
    "        if line.startswith(\"#\"):\n",
    "            all_sentence_ids.append(re.split(\"\\\\s+\", line.strip())[1])\n",
    "            continue\n",
    "        if not line.strip():\n",
    "            all_sentences.append(sentence)\n",
    "            all_labels.append(labels)\n",
    "            sentence = []\n",
    "            labels = []\n",
    "        else:\n",
    "            line = line.strip()\n",
    "            sentence.append(re.split(\"\\\\s+\", line)[0])\n",
    "            labels.append(re.split(\"\\\\s+\", line)[1])\n",
    "    if sentence and labels:\n",
    "        all_sentences.append(sentence)\n",
    "        all_labels.append(labels)\n",
    "    return all_sentence_ids, all_sentences, all_labels\n",
    "\n",
    "\n",
    "def shorten_sentence_label(sentence_tokens, true_labels, pred_labels, maxlen):\n",
    "    if maxlen == -1: # we need to shorten the labels to the sentence length\n",
    "        shorten_to = len(sentence_tokens)\n",
    "    else: # we have to shorten either the sentence to the max len or the sequence to the sentence length\n",
    "        if len(sentence_tokens) > maxlen:\n",
    "            shorten_to = maxlen\n",
    "        else:\n",
    "            shorten_to = len(sentence_tokens)\n",
    "    return sentence_tokens[:shorten_to], true_labels[:shorten_to], pred_labels[:shorten_to]\n",
    "\n",
    "\n",
    "\n",
    "def generate_conll(sentence_ids,all_sentence_tokens, all_sentence_true_labels, all_sentence_pred_labels, output_base, \n",
    "                   maxlen=-1):\n",
    "    assert len(sentence_ids) == len(all_sentence_tokens) == len(all_sentence_true_labels) == len(all_sentence_pred_labels)\n",
    "    with open(f\"{output_base}.conll\", \"w\") as wf:\n",
    "        for sentence_tokens, sentence_true_labels, sentence_pred_labels in zip(all_sentence_tokens, all_sentence_true_labels, all_sentence_pred_labels):\n",
    "            sentence_tokens, sentence_true_labels, sentence_pred_labels = shorten_sentence_label(\n",
    "                sentence_tokens, sentence_true_labels, sentence_pred_labels, maxlen)\n",
    "            assert len(sentence_tokens) == len(sentence_true_labels) == len(sentence_pred_labels), \\\n",
    "            f\"{len(sentence_tokens)}, {len(sentence_true_labels)}, {len(sentence_pred_labels)}\"\n",
    "            for token, true_label, pred_label in zip(sentence_tokens, sentence_true_labels, sentence_pred_labels):\n",
    "                wf.write(f\"{token} {true_label} {pred_label}\\n\")\n",
    "                wf.write(\"\\n\")\n",
    "    print(f\"generated conll file {output_base}.conll\")\n",
    "\n",
    "def generate_labelseq(sentence_ids, all_sentence_tokens, all_sentence_pred_labels, output_base, maxlen=-1):\n",
    "    assert len(sentence_ids) == len(all_sentence_tokens) == len(all_sentence_pred_labels)\n",
    "    with open(f\"{output_base}_labelseq.txt\", \"w\") as wf:\n",
    "        wf.write(\"ID\\tTAGSEQ\\n\")\n",
    "        for sentence_id, sentence_tokens, sentence_labels in zip(sentence_ids, all_sentence_tokens, all_sentence_pred_labels):\n",
    "            sentence_tokens, _, sentence_labels = shorten_sentence_label(sentence_tokens, sentence_labels, sentence_labels, maxlen)\n",
    "            assert len(sentence_tokens) == len(sentence_labels)\n",
    "            wf.write(f'{sentence_id}\\t{\" \".join(sentence_labels)}\\n')\n",
    "        print(f\"generated labelseq file {output_base}.labelseq\")\n",
    "\n",
    "def generate_human_readable(sentence_ids, all_sentence_tokens, all_sentence_pred_labels, output_base, maxlen=-1):\n",
    "    def create_chunk(tokens, chunk_def):\n",
    "            chunk_type, chunk_indices = chunk_def.split(\"@\")[0], [int(x) for x in chunk_def.split(\"@\")[1:]]\n",
    "            chunk_indices = chunk_indices + [chunk_indices[-1]+1]\n",
    "            # return f\"{chunk_type}: {' '.join(tokens[chunk_indices[0]: chunk_indices[-1]])}\"\n",
    "            return f\"{' '.join(tokens[chunk_indices[0]: chunk_indices[-1]])}\"\n",
    "\n",
    "    assert len(sentence_ids) == len(all_sentence_tokens) == len(all_sentence_pred_labels)\n",
    "    with open(f\"{output_base}_human.txt\", \"w\") as wf:\n",
    "        wf.write(\"Term\\n\")\n",
    "        for sentence_id, sentence_tokens, sentence_labels in zip(sentence_ids, all_sentence_tokens, all_sentence_pred_labels):\n",
    "            # wf.write(f\"[id]: {sentence_id}\\n\")\n",
    "            # wf.write(f\"{sentence_id}\\t\")\n",
    "            sentence_tokens, _, sentence_labels = shorten_sentence_label(sentence_tokens, sentence_labels, sentence_labels, maxlen)\n",
    "            assert len(sentence_tokens) == len(sentence_labels)\n",
    "            # wf.write(f\"[sentence]: {' '.join(sentence_tokens)}\")\n",
    "            chunks = to_chunks(sentence_labels, span_type=\"iob\") \n",
    "            for chunk in chunks:\n",
    "                wf.write(create_chunk(sentence_tokens, chunk)+ \", \")\n",
    "            wf.write(\"\\n\")\n",
    "        print(f\"generated labelseq file {output_base}.human\")\n",
    "\n",
    "\n",
    "def predict_tags_for_file(_file, model, _word2idx, _label2idx, output_base, output_formats=[\"human_readable\", \"labelseq\"]):\n",
    "    sentence_ids, sen_texts = read_file(_file)\n",
    "    X = [[word2idx[j] for j in i] for i in sen_texts]\n",
    "    X = pad_sequences(maxlen = max(len(x) for x in sen_texts), sequences = X, padding = \"post\", value = _word2idx[\"PAD\"])\n",
    "    \n",
    "    Y_pred = np.argmax(model.predict(X), axis=-1)\n",
    "    \n",
    "    Y_pred_labels = [[idx2label[i] for i in row] for row in Y_pred]\n",
    "    if \"labelseq\" in output_formats:\n",
    "        generate_labelseq(\n",
    "            sentence_ids=sentence_ids,\n",
    "            all_sentence_tokens=sen_texts, \n",
    "            all_sentence_pred_labels=Y_pred_labels, \n",
    "            output_base=output_base,\n",
    "            maxlen=MAXLEN\n",
    "        )\n",
    "    if \"human_readable\" in output_formats:\n",
    "         generate_human_readable(\n",
    "            sentence_ids=sentence_ids,\n",
    "            all_sentence_tokens=sen_texts, \n",
    "            all_sentence_pred_labels=Y_pred_labels, \n",
    "            output_base=output_base,\n",
    "            maxlen=MAXLEN\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcErfEePhEcT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"review_data\"):\n",
    "    !wget https://www.dropbox.com/s/yqgff7de73iwosr/review_data.zip?dl=1 -O review_data.zip\n",
    "    !unzip review_data.zip\n",
    "    !ls review_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4dn1aqiClN9P",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "id, X = read_file(\"review_data/REVIEW_TEXT.txt\")\n",
    "id, y = read_file(\"review_data/REVIEW_LABELSEQ.txt\") \n",
    "id, test_text = read_file(\"review_data/TEST_REVIEW_TEXT.txt\") \n",
    "\n",
    "train_text, valid_text, train_labels, valid_labels = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHSh_Y8khEcT",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data preprocessing, Sequence padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzjGFTxlhEcT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "unique_words = set([j for i in train_text + valid_text + test_text for j in i])\n",
    "word2idx = {j:i+1 for i,j in enumerate(unique_words)}\n",
    "word2idx[\"PAD\"] = 0\n",
    "print(f\"{len(word2idx)} tokens in vocab\")\n",
    "\n",
    "unique_labels = set([j for i in train_labels for j in i])\n",
    "unique_labels_valid = set([j for i in train_labels for j in i])\n",
    "unique_labels_test = set([j for i in train_labels for j in i])\n",
    "\n",
    "# make sure there are no labels in valid/test that are not in train.\n",
    "assert not unique_labels_valid - unique_labels, unique_labels_valid - unique_labels\n",
    "assert not unique_labels_test - unique_labels, unique_labels_test - unique_labels\n",
    "\n",
    "label2idx = {'PAD': 0}\n",
    "for i,j in enumerate(unique_labels):\n",
    "    label2idx[j] = i+1 \n",
    "idx2label = {j:i for i,j in label2idx.items()}\n",
    "print(idx2label)\n",
    "\n",
    "MAXLEN = max(len(x) for x in X)\n",
    "\n",
    "def get_padded_x_y(text, labels, _maxlen, _word2idx, _label2idx):\n",
    "    X = [[word2idx[j] for j in i] for i in text]\n",
    "    X = pad_sequences(maxlen = _maxlen, sequences = X, padding = \"post\", value = _word2idx[\"PAD\"])\n",
    "    Y = [[label2idx[j] for j in i] for i in labels]\n",
    "    Y = pad_sequences(maxlen = _maxlen, sequences = Y, padding = \"post\", value = _label2idx[\"PAD\"])\n",
    "    Y = [to_categorical(i, num_classes = len(label2idx)) for i in Y]\n",
    "    assert len(X) == len(Y), \"X and Y should be of the same shape\"\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = get_padded_x_y(train_text, train_labels, _maxlen=MAXLEN, _word2idx=word2idx, _label2idx=label2idx)\n",
    "X_valid, Y_valid = get_padded_x_y(valid_text, valid_labels, _maxlen=MAXLEN, _word2idx=word2idx, _label2idx=label2idx)\n",
    "# X_test, Y_test = get_padded_x_y(test_text, test_labels, _maxlen=MAXLEN, _word2idx=word2idx, _label2idx=label2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XH941rgChEcU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### LSTM model (without embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOfXKFO9hEcU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "EMBED_DIM = 300\n",
    "RNN_HIDDEN_DIM = 300\n",
    "model.add(Embedding(input_dim=len(word2idx.keys()),output_dim=EMBED_DIM,input_length=MAXLEN))\n",
    "model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.1), merge_mode = 'concat'))\n",
    "model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.13), merge_mode = 'concat'))\n",
    "model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.16), merge_mode = 'concat'))\n",
    "model.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.2), merge_mode = 'concat'))\n",
    "model.add(Dense(len(label2idx.keys()), activation=\"softmax\"))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4hiMscQNhEcU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# history = model.fit(X_train,np.array(Y_train),batch_size=16,epochs=3,validation_data=(X_valid, np.array(Y_valid)))\n",
    "history = model.fit(X_train, np.array(Y_train), batch_size=16, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xsqsBpIYhEcU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_valid_pred = model.predict(X_valid)\n",
    "Y_valid_pred = np.argmax(Y_valid_pred, axis=-1)\n",
    "Y_valid_true = np.argmax(Y_valid, -1)\n",
    "Y_valid_pred_labels = [[idx2label[i] for i in row] for row in Y_valid_pred]\n",
    "Y_valid_true_labels = [[idx2label[i] for i in row] for row in Y_valid_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mWQ9cfZGhEcU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "labels = ['B-AE','B-SSI','I-AE', 'I-SSI', 'O']\n",
    "\n",
    "report = flat_classification_report(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels,\n",
    "                                    labels = labels)\n",
    "f1_score = flat_f1_score(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels, average = 'macro', labels = labels)\n",
    "print(f'Model 7\\nF1_Score (macro): {f1_score:.3f}\\nClassification report:\\n{report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ObtpHfaGhEcU",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batchsz = 16\n",
    "\n",
    "predict_tags_for_file(\"review_data/TEST_REVIEW_TEXT.txt\", model=model, _label2idx=label2idx, _word2idx=word2idx, output_base=\"test_output\", \n",
    "                      output_formats=[\"human_readable\", \"labelseq\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36SpH-7rVMyj",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### try word embeddings (glove-840b-300d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oLjIV2IaVMKF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!gdown 1-2HyX0Ak1rOKa7UG6_mfnZY0Lvk-NTwz\n",
    "path_to_glove_file = 'glove.840B.300d.txt'\n",
    "\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g_YMcm3KVvUh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_tokens = len(word2idx)\n",
    "embedding_dim = 300\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    try:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    except:\n",
    "        misses += 1\n",
    "        pass\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "# some words generated nan embeddings. \n",
    "# Check for nan values and fill with 0\n",
    "check_nan = np.isnan(embedding_matrix)\n",
    "embedding_matrix[check_nan] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8BjRgqSWT5v",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_emb = Sequential()\n",
    "EMBED_DIM = 300\n",
    "RNN_HIDDEN_DIM = 300\n",
    "model_emb.add(Embedding(input_dim=len(word2idx.keys()),output_dim=EMBED_DIM,input_length=MAXLEN,\n",
    "                    embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=False))\n",
    "model_emb.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.2), merge_mode = 'concat'))\n",
    "model_emb.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.16), merge_mode = 'concat'))\n",
    "model_emb.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.13), merge_mode = 'concat'))\n",
    "model_emb.add(Dense(len(label2idx.keys()), activation=\"softmax\"))\n",
    "\n",
    "model_emb.compile(loss='categorical_crossentropy', optimizer= \"adam\", metrics=['acc'])\n",
    "model_emb.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "colJxMWXcUVV",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model_emb.fit(X_train, np.array(Y_train), batch_size=16, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E7W1sdLod0H8",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_valid_pred = model_emb.predict(X_valid)\n",
    "Y_valid_pred = np.argmax(Y_valid_pred, axis=-1)\n",
    "Y_valid_true = np.argmax(Y_valid, -1)\n",
    "Y_valid_pred_labels = [[idx2label[i] for i in row] for row in Y_valid_pred]\n",
    "Y_valid_true_labels = [[idx2label[i] for i in row] for row in Y_valid_true]\n",
    "\n",
    "labels = ['B-AE','B-SSI','I-AE', 'I-SSI', 'O']\n",
    "\n",
    "report = flat_classification_report(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels,\n",
    "                                    labels = labels)\n",
    "f1_score = flat_f1_score(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels, average = 'macro', labels = labels)\n",
    "print(f'Model 9\\nF1_Score (macro): {f1_score:.3f}\\nClassification report:\\n{report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Di7OGOzlz1Vb",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Try word embedding (BioWordVec) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1qVjVQvldsGn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!wget https://ftp.ncbi.nlm.nih.gov/pub/lu/Suppl/BioSentVec/BioWordVec_PubMed_MIMICIII_d200.vec.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l023yOTRhlcs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert binary word embedding file to txt file (this cell may take ~30 mins to run in colab environment)\n",
    "\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "path = \"/content/BioWordVec_PubMed_MIMICIII_d200.vec.bin\"\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "model.save_word2vec_format(\"BioWordVec.txt\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zWsh8yeSriqT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(\"BioWordVec.txt\"):\n",
    "    !wget https://www.dropbox.com/s/yqgff7de73iwosr/review_data.zip?dl=1 -O review_data.zip\n",
    "    !unzip review_data.zip\n",
    "    !ls review_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNIf-xfGp5dW",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "path = 'BioWordVec.txt'\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nfDf4vBOuMKS",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "num_tokens = len(word2idx)\n",
    "embedding_dim = 200 #biowordvec is 200d\n",
    "hits = 0\n",
    "misses = 0\n",
    "\n",
    "# Prepare embedding matrix\n",
    "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
    "for word, i in word2idx.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    try:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        hits += 1\n",
    "    except:\n",
    "        misses += 1\n",
    "        pass\n",
    "print(\"Converted %d words (%d misses)\" % (hits, misses))\n",
    "\n",
    "# some words generated nan embeddings. \n",
    "# Check for nan values and fill with 0\n",
    "check_nan = np.isnan(embedding_matrix)\n",
    "embedding_matrix[check_nan] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ugwn-66Mw4pL",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_emb = Sequential()\n",
    "EMBED_DIM = 200\n",
    "RNN_HIDDEN_DIM = 300\n",
    "model_emb.add(Embedding(input_dim=len(word2idx.keys()),output_dim=EMBED_DIM,input_length=MAXLEN,\n",
    "                    embeddings_initializer=keras.initializers.Constant(embedding_matrix), trainable=True))\n",
    "model_emb.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.2), merge_mode = 'concat'))\n",
    "model_emb.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.16), merge_mode = 'concat'))\n",
    "model_emb.add(Bidirectional(LSTM(units=RNN_HIDDEN_DIM,return_sequences=True,dropout=0.13), merge_mode = 'concat'))\n",
    "model_emb.add(Dense(len(label2idx.keys()), activation=\"softmax\"))\n",
    "\n",
    "model_emb.compile(loss='categorical_crossentropy', optimizer= \"adam\", metrics=['acc'])\n",
    "model_emb.summary()\n",
    "\n",
    "history = model_emb.fit(X_train, np.array(Y_train), batch_size=16, epochs=10, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OA2rq7w7xuV-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Y_valid_pred = model_emb.predict(X_valid)\n",
    "Y_valid_pred = np.argmax(Y_valid_pred, axis=-1)\n",
    "Y_valid_true = np.argmax(Y_valid, -1)\n",
    "Y_valid_pred_labels = [[idx2label[i] for i in row] for row in Y_valid_pred]\n",
    "Y_valid_true_labels = [[idx2label[i] for i in row] for row in Y_valid_true]\n",
    "\n",
    "labels = ['B-AE','B-SSI','I-AE', 'I-SSI', 'O']\n",
    "\n",
    "report = flat_classification_report(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels,\n",
    "                                    labels = labels)\n",
    "f1_score = flat_f1_score(y_pred=Y_valid_pred_labels, y_true=Y_valid_true_labels, average = 'macro', labels = labels)\n",
    "print(f'Model 11\\nF1_Score (macro): {f1_score:.3f}\\nClassification report:\\n{report}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6MbgjuTjEWwC",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### generate final prediction (model used: Glove embedding) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpTAmI4fgwU-",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "batchsz = 16\n",
    "\n",
    "# human readable has been modified to include only texts from the prediction made\n",
    "predict_tags_for_file(\"review_data/TEST_REVIEW_TEXT.txt\", model=model_emb, _label2idx=label2idx, _word2idx=word2idx, output_base=\"test_output\", \n",
    "                      output_formats=[\"human_readable\", \"labelseq\"])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "private_outputs": true,
   "provenance": [
    {
     "file_id": "1nQeota2pMur0bGNGcYuGFfaf8nO6o5WQ",
     "timestamp": 1679677122447
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
